{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72167493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e677e242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import time\n",
    "\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate import load_checkpoint_and_dispatch\n",
    "from transformers.generation.streamers import BaseStreamer\n",
    "\n",
    "from model.block_transformer import BlockTransformer\n",
    "from model.utils import load_block_transformer_from_config, load_vanilla_model_from_config\n",
    "from paths import PROJECT_ROOT\n",
    "from util.config import load_config\n",
    "from util.tokenizer import TOKENIZERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66d7d340",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_ROOT = os.path.join(PROJECT_ROOT, \"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e34ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_path(name):\n",
    "    if \".yaml\" not in name:\n",
    "        name += \".yaml\"\n",
    "    return os.path.join(PROJECT_ROOT, \"conf\", \"trainer\", name)\n",
    "\n",
    "\n",
    "def get_checkpoint_path(name):\n",
    "    root = os.path.join(CHECKPOINT_ROOT, name)\n",
    "    if not os.path.exists(root):\n",
    "        raise ValueError(f\"Checkpoint directory does not exist: {root}\")\n",
    "    pattern = os.path.join(CHECKPOINT_ROOT, name, \"checkpoint-*\")\n",
    "    checkpoint_paths = glob.glob(pattern)\n",
    "    def get_step(checkpoint_path):\n",
    "        bs = os.path.basename(checkpoint_path)\n",
    "        return int(bs.split(\"-\")[1])\n",
    "    checkpoint_paths = [(get_step(cp), cp) for cp in checkpoint_paths]\n",
    "    checkpoint_paths.sort()\n",
    "    checkpoint_path = checkpoint_paths[-1][1]\n",
    "    checkpoint_path = os.path.join(checkpoint_path, \"model.safetensors\")\n",
    "    print(f\"Retrieving latest checkpoint path {checkpoint_path}\")\n",
    "    return checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa6418c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(name, block=True):\n",
    "    config = load_config(get_config_path(name))\n",
    "    with init_empty_weights():\n",
    "        if block:\n",
    "            model, tokenizer = load_block_transformer_from_config(config)\n",
    "        else:\n",
    "            model = load_vanilla_model_from_config(config)\n",
    "    checkpoint = get_checkpoint_path(name)\n",
    "    device_map = \"sequential\"  # set to auto to use multiple GPUs + pipelining (not tested)\n",
    "    model = load_checkpoint_and_dispatch(model, checkpoint=checkpoint, device_map=device_map)\n",
    "    if block:\n",
    "        return model, tokenizer\n",
    "    else:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b56b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_temperature(model, temperature):\n",
    "    if isinstance(model, BlockTransformer):\n",
    "        model.token_decoder.generation_config.update(do_sample=True, temperature=temperature)\n",
    "    else:\n",
    "        model.generation_config.update(do_sample=True, temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e031ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstSampleStreamer(BaseStreamer):\n",
    "    def __init__(self, tokenizer, escape_newline=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.first = True\n",
    "        self.escape_newline = escape_newline\n",
    "        \n",
    "    def put(self, value):\n",
    "        # ignore prompt\n",
    "        if self.first:\n",
    "            self.first = False\n",
    "            return\n",
    "        token = tokenizer.decode(value[-1])\n",
    "        if self.escape_newline:\n",
    "            token = token.replace(\"\\n\", \"\\\\n\")\n",
    "        print(token, end=\"\", flush=True)\n",
    "    \n",
    "    def end(self):\n",
    "        self.first = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850739e2",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcb4c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way—in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.\n",
    "\n",
    "There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France. In both countries it was clearer than crystal to the lords of the State preserves of loaves and fishes, that things in general were settled for ever.\n",
    "\n",
    "It was the year of Our Lord one thousand seven hundred and seventy-five. Spiritual revelations were conceded to England at that favoured period, as at this. Mrs. Southcott had recently attained her five-and-twentieth blessed birthday, of whom a prophetic private in the Life Guards had heralded the sublime appearance by announcing that arrangements were made for the swallowing up of London and Westminster. Even the Cock-lane ghost had been laid only a round dozen of years, after rapping out its messages, as the spirits of this very year last past (supernaturally deficient in originality) rapped out theirs. Mere messages in the earthly order of events had lately come to the En- glish Crown and People, from a congress of British subjects in America: which, strange to relate, have proved more important to the human race than any communications yet received through any of the chickens of the Cock-lane brood.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "632b691a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt length: 417\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TOKENIZERS[\"pythia\"]\n",
    "prompt_length = len(tokenizer(prompt)[\"input_ids\"])\n",
    "print(\"Prompt length:\", prompt_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bfaa73",
   "metadata": {},
   "source": [
    "## Block Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1094aff0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itsnamgyu/block-transformer/util/config.py:78: UserWarning: stop_steps (286000) is not divisible by save_steps (5000)\n",
      "  warnings.warn(warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ Preprocess Config -------------------------------\n",
      "Automatically determining batch size based on `total_batch_size`\n",
      "total_batch_size              : 512 (given)\n",
      "torch.cuda.device_count()     : 1\n",
      "per_device_train_batch_size   : 16 (given)\n",
      "gradient_accumulation_steps   : 32 (computed)\n",
      "actual total batch size       : 512\n",
      "Setting wandb_run_name    : block_main_b4_1.2b\n",
      "Setting output_dir        : block_main_b4_1.2b\n",
      "Using deepspeed config    : /home/itsnamgyu/block-transformer/ds_configs/default_linear_warmup.config\n",
      "[token_decoder] Setting num_attention_heads to hidden_size // 128\n",
      "[token_decoder] Setting intermediate_size to hidden_size * 4\n",
      "[block_decoder] Setting num_attention_heads to hidden_size // 128\n",
      "[block_decoder] Setting intermediate_size to hidden_size * 4\n",
      "--------------------------------------------------------------------------------\n",
      "Initializing block decoder from scratch...\n",
      "Using custom config for block decoder...\n",
      "   num_hidden_layers: 12\n",
      "   hidden_size: 2048\n",
      "   num_attention_heads: 16\n",
      "   intermediate_size: 8192\n",
      "Initializing embedder from scratch...\n",
      "Using custom config for embedder...\n",
      "   vocab_size: 50304\n",
      "   hidden_size: 512\n",
      "Initializing token decoder from scratch...\n",
      "Using custom config for token decoder...\n",
      "   num_hidden_layers: 12\n",
      "   hidden_size: 2048\n",
      "   num_attention_heads: 16\n",
      "   intermediate_size: 8192\n",
      "Retrieving latest checkpoint path /home/itsnamgyu/block-transformer/results/block_main_b4_1.2b/checkpoint-285000/model.safetensors\n"
     ]
    }
   ],
   "source": [
    "block, tokenizer = load_model(\"block_main_b4_1.2b\", block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a475020",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A few weeks before the close of our year, a party of persons were travelling from Liverpool and Yorkshire to Exeter. The number that got away must have been a respectable one, for there was plenty of luggage and everything of the kind in readiness to give ease at great resorts. On these occasions the traveller’s guide of choice often was the newspaper advertisement of the railway companies, which contains a general view of the country in a variety of well-known places, and offers many opportunities for information.\n",
      "\n",
      "Among the notices are advertisements describing the beauties which are to be seen on each side of our journey, and describing all the places for excursion. There is an advertisement for the Exeter Railway, and another for that of the London and North Eastern Railway, to serve as the latter’s link with the great northern line between Birmingham and London. “We give this as a general view,” the advertisement says, “but we must let some of our readers see that such of our little towns as we shall describe can be reached by the quickest run which passes through this most populous city.” And a little farther on we are told that “We find that the Exeter Railway is the most remarkable railway between here and Yorkshire, and in many ways no less so. Exeter may be called the greatest railway junction, and in truth, we think not. To us Exeter is a city of much promise, and is a very attractive town. In a word, it must be a place in our choice, both as to railway travel and place of residence. The whole way we mean from Exeter to London is about seventy miles long. In addition to the railway trains that traverse it, we give the following as an excursion on this train.”\n",
      "\n",
      "The advertisement for the Exeter was sent to the London and South Eastern Railway line as well as to the Lake and South Western Railway with an additional announcement to the inhabitants on that portion of it as far as Stretton.\n",
      "\n",
      "Another announcement is on “The Exeter and South Pembroke Railway.” “This is our line to give pleasure travelling parties,” the advertisement says. Its advertisement says that passengers could find plenty of amusement in the railway line itself, including a return visit to the former port, a visit to which our excursionist does not disapprove in the least.\n",
      "\n",
      "These advertisements are not the only newspaper advertisements, but the various other publications, newspapers, magazines, and periodicals, which, in the last days of the year, reach any part of the empire, are equally valuable in the department of literature to those more general advertisements to which, by reason of their being so various, the railways are but imperfectly connected.\n",
      "\n",
      "\n",
      "\n",
      "If we are not, as at present far advanced, in the list of visitors for this winter, there is a good chance, perhaps, that you will make the same remark, that we have seen as our fair share of these holidays. I do assure you, however, your own experience, which we all know cannot equal that of others, will show the truth of these assertions. I do not expect any more interesting descriptions of the Exeter railway this winter. It may be described as a regular and commodious town, and will be found to possess all the conveniences that are indispensable to its nature.\n",
      "\n",
      "As a result of this advertisement, it would be well if Exeter readers of the Express should be pleased to avail themselves of a line ticket to that station. The time will be shortly at hand when it will be most convenient for travellers to journey to that station. It is now twenty-four hours’ travel to London, and eight in our run to Liverpool. The journey time for each train is twenty-five minutes; and at every station there are regular trains waiting to take the passenger along that line, either direct from Exeter, or from the station at Exeter, or from the station at Exeter, going straight on to London. In brief, as far as the present line extends, it can be readily made to London.\n",
      "\n",
      "\n",
      "\n",
      "We have all the advantages of the season in our streets and houses, for the benefit of which we have all taken great pains. We have made an agreeable selection, and we want you to have nothing from our friends. A very little more money may be spared if any one will spend a half-year’s wages to-morrow in getting together that amount.\n",
      "\n",
      "We are indebted to your letter for many further pieces, many of which we have not yet had a chance to say in this article, as it is for the most part so free from the general current of that literature in which it is written, that it may, in the very nature of its purpose, be free from much of the exaggerations and inaccuracies which are in consequence more or less prevalent. A long article may be quoted to you on the subject of the railway. We only suggest that, if it is desirable to be given to the public, that it should be given in the form of a monthly paper, in which it would not be thought necessary to insert any fictitious assertions. In describing railways by the English Railway Companies we should only, therefore, be describing what might be said about other trains. And if you do not agree with us on any particular point, but you believe that the railways are all alike, that the trains are all alike, then have no cause for complaint. We trust that your remarks on this occasion will be valuable.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***<|endoftext|>"
     ]
    }
   ],
   "source": [
    "set_temperature(block, temperature=1)\n",
    "streamer = FirstSampleStreamer(tokenizer)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = {k: t.cuda() for k, t in inputs.items()}\n",
    "output_ids = block.generate(**inputs, max_length=2048, streamer=streamer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be357ba3",
   "metadata": {},
   "source": [
    "## Vanilla Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3454a405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ Preprocess Config -------------------------------\n",
      "Automatically determining batch size based on `total_batch_size`\n",
      "total_batch_size              : 256 (given)\n",
      "torch.cuda.device_count()     : 1\n",
      "per_device_train_batch_size   : 8 (given)\n",
      "gradient_accumulation_steps   : 32 (computed)\n",
      "actual total batch size       : 256\n",
      "Setting wandb_run_name    : vanilla_410\n",
      "Setting output_dir        : vanilla_410\n",
      "Using deepspeed config    : /home/itsnamgyu/block-transformer/ds_configs/default_linear_warmup.config\n",
      "--------------------------------------------------------------------------------\n",
      "Initializing model from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itsnamgyu/block-transformer/util/config.py:78: UserWarning: stop_steps (572000) is not divisible by save_steps (10000)\n",
      "  warnings.warn(warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving latest checkpoint path /home/itsnamgyu/block-transformer/results/vanilla_410/checkpoint-570000/model.safetensors\n"
     ]
    }
   ],
   "source": [
    "vanilla = load_model(\"vanilla_410\", block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c1da58c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- Start of generation ------------------------------\n",
      " The year had just been celebrated by an exhibition in the West End of the City, to-be terminated by a public dinner. (Mr. Rack-ford and Mrs. Rack-ford were going to the theatre to-day with their cousin, Mr. Wainwright, to watch the play of Molière; Mr. Wainwright was just now going from one of the houses of the East End to another, into the open air.)\n",
      "\n",
      "That very curious old woman had recently fallen on her knees before the altar of St. Patrick's, with a litany of prayers, and received the sacred sacrament. A great deal of the parish seemed now ready for an altar, in the form of a chapel or a pulpit. A large square room, as long and deep, as high and sharp, and so commodious in its shape, was just at hand. A small fountain, of a certain colour, in front of the altar, had, under a fine gilt inscription, been used to mark the hour of service. The new chapel, in the mean time, had been a house of worship: and, after a time, these inhabitants would have been baptized, as they were now, in the holy waters. A large number of the faithful would, at such times, have prayed in the chapel itself, as they did at others—when the solemn hymns of the psalms were sung over them.\n",
      "\n",
      "That very young Lady Rackford (who was very young at the time to be celebrated) had been just then in her little room, in which her maid was waiting a letter, written by herself to her brother-in-law, who was staying in London with her, from America. The letter would have to be read in her apartment—so it appeared from the first. A servant, on entering, was announced, and the lady of the house (and she seemed, indeed, very neat and well adjusted) appeared with a little note in her hand, written directly upon it, the contents of which she had lately been to her brother-in-law's house—in which letter, after referring to certain dates, which would soon, or not very soon, be seen, she expressed herself, with great diffidence, in a very tender and humble manner, and said, that she very much wished to thank the Lord for his kindness to her, both in England and in America.\n",
      "\n",
      "In that letter, before its execution, she had not only sent to her brother-in-law the following lines, which she had herself written, but a line or two which she herself had intended would have been executed—or she had intended she would, by the means which she had in her power, have communicated, with a view to the receiving of the divine gift; and, from a strange variety of circumstances, she had herself written, by her own direction, as it might seem, upon paper, not yet the precious and necessary document, but the ordinary paper, which, in the first instances, is very common—a paper which, in like cases, is sometimes carried off, in a manner different from that in which its proper execution might be expected to be performed. The letter and letter-press—and even letters, as well, were sometimes brought away, with a view of sending—had long been known to be the property of her brother-in-law. They had long been kept as such; and Lady Rackford was ever ready and anxious to hear from her brother-in-law—who had at this time, as usual, been at a very intimate and distant meeting—a letter from him written, in all haste, in foreign tongues; and which she was at this moment ready, with all her heart, to receive at the church and chapel of St. Augustine's.\n",
      "\n",
      "There remained, indeed, only one letter in which the first part was written, and the second part omitted; for, the first part being written by the most particular and tender and humble of her own heart, and the second by the most particular and tender and mysterious hand of her brother, and was almost the copy of the letters, which had once been written and written in other lands, and the other letters, which had once been heard and written abroad, had in common been long lost, and were now, in the hands of strangers, only the ordinary piece of paper, which a book could not print or speak forth.\n",
      "\n",
      "Of the first part it was understood, that that part of her letter, which was to be received in her apartment, was for giving instruction among her domestics.\n",
      "\n",
      "After she had read, in the first instance, Mr. Rackford's lines—(which were an exaltation and a rejoicing of spirit, as much as to say, \"All has been done indeed to meet my wishes; the past is a long history, and much must be done for me!\") she read the second part with much attention, while one of the domestics was standing, just outside, with a pen poised in her hand, and writing it. The letter was a very curious letter—not one to be laughed at, when, in her own domestic life, no house was ever her own or had a right to her name, and even when an English woman, and woman in London at that day, was considered in most families to be as much a part and property of those houses as a piece of plate or a piece of woodwork were to a house or its property. The letter had been written in a long time, a long time; and was now not so young to be written—young, that it is said, can now not bear life unnaturally long. Nor had it been written at the time when all its subject-matter had already been so very well known. It had been written when she herself was young, and was to be, as yet, written when she herself had been twenty. Now it was written, not at that time, and, indeed, very long ago—and seemed very long ago—at that very moment. How long ago? A certain distance, in years, the age of the world—that is to say, the time of our day—had so gone over us all together, that, now, there was a time when we were no longer as young as we are, and we went as we were; and in that time, her husband—who, in England, was now twenty-four—had married at twenty-eight, and had had his children now at sixteen. The letter that she was to deliver, would have been opened in two hours at a place near the parish church of St. Mary-with-the-bell, and would have been published the same day. All this, however, had been done in a country town in a county that was now called in that letter, and had been then the kingdom of Ireland.\n",
      "\n",
      "And in another country town, not yet a kingdom, she herself lived at the same time—a town she called \"her kingdom,\" which was called in this letter, a town called \"she\" that was called in this letter—a town as lovely to-day as it had ever been to-day. But it was beautiful in her eyes as it was beautiful to-day—as beautiful as it seemed to be to-day.\n",
      "\n",
      "She had never been a child, nor ever thought of being a child; and she had not thought of being a child.\n",
      "\n",
      "\"They tell me I cannot live without food and drink for the next forty-eight hours. How d'ye do?\"\n",
      "\n",
      "She had never before heard a question—never been questioned by a question about anything—in the course of a conversation that was, or would be, always perfectly simple and innocent. The question that she was in the habit of asking herself, (in order to make no unnecessary remarks, and by no"
     ]
    }
   ],
   "source": [
    "set_temperature(vanilla, temperature=1)\n",
    "streamer = FirstSampleStreamer(tokenizer)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = {k: t.cuda() for k, t in inputs.items()}\n",
    "print(\" Start of generation \".center(80, \"-\"))\n",
    "start = time.time()\n",
    "output_ids = vanilla.generate(**inputs, max_length=2048, streamer=streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edacb226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
