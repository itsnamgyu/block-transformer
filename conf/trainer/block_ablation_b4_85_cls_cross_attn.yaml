name: block_ablation_b4_85_cls_cross_attn

wandb: true
wandb_project: block_transformer
wandb_entity: block-transformer
wandb_run_name: null
output_dir: null

wandb_run_id: null
resume_from_checkpoint: false

pythia_pile_idxmaps_path: /data/pythia/pythia_pile_idxmaps
dataset: pythia_pile
block_length: 4
block_split:
  distribution: fixed
  distribution_kwargs:
    length: 4
total_batch_size: 256
per_device_train_batch_size: 32
gradient_accumulation_steps: null
batch_size_rampup_steps: null
max_length: 2048

tokenizer:
  embedder: pythia
  token_decoder: pythia

embedder:
  cls: roberta_cls
  model_name_or_path: roberta-base
  use_pretrained_weights: False
  n_cls_tokens: 3
  n_embedding_tokens: 1
  projection_method: null
  config:
    num_hidden_layers: 3
    hidden_size: 256
    vocab_size: 50304

block_decoder:
  cls: gpt-neo-x
  model_name_or_path: EleutherAI/pythia-410m-deduped
  use_pretrained_weights: False
  loss_name: null
  attn_implementation: flash_attention_2
  config:
    num_hidden_layers: 6
    hidden_size: 768

token_decoder:
  cls: t5
  model_name_or_path: t5-small
  use_pretrained_weights: False
  expansion_method: expansion_layer
  decoding_strategy: cross_attention
  config:
    d_model: 768
    d_ff: 3072
    num_layers: 6
    num_decoder_layers: 6
    num_heads: 12  # if d_model is 640, num_heads are set to 10. 
    vocab_size: 50304

token_decoding_loss:
  enable: true

block_decoding_loss:
  enable: false
  weight: 1.0

auto_encoding_loss:
  enable: false
  weight: 1.0

adaptive_lr_auto_encoder: null

freeze: false
learning_rate: 6e-4
adam_beta1: 0.9
adam_beta2: 0.95
weight_decay: 0.1
precision: bf16
num_train_steps: 572000
stop_steps: 50000
num_warmup_steps: 3000
save_steps: 5000
save_total_limit: null
logging_steps: 200
dataloader_num_workers: 8

deepspeed: ds_configs/default_linear_warmup.config
