name: block_main_b4_800

wandb: true
wandb_project: block_transformer
wandb_entity: block-transformer
wandb_run_name: null
output_dir: null

wandb_run_id: null
resume_from_checkpoint: false

pythia_pile_idxmaps_path: /data/pythia/pythia_pile_idxmaps
dataset: pythia_pile
block_length: 4
block_split:
  distribution: fixed
  distribution_kwargs:
    length: 4
total_batch_size: 512
per_device_train_batch_size: 16
gradient_accumulation_steps: null
batch_size_rampup_steps: null
max_length: 2048

tokenizer:
  embedder: pythia
  token_decoder: pythia

embedder:
  cls: lookup
  model_name_or_path: null
  n_embedding_tokens: 1
  use_pretrained_weights: False
  projection_method: concat
  config:
    vocab_size: 50304
    hidden_size: 512

block_decoder:
  cls: gpt-neo-x
  model_name_or_path: EleutherAI/pythia-410m-deduped
  use_pretrained_weights: False
  loss_name: null
  attn_implementation: flash_attention_2
  config:
    num_hidden_layers: 8
    hidden_size: 2048

token_decoder:
  cls: gpt-neo-x
  model_name_or_path: EleutherAI/pythia-410m-deduped
  use_pretrained_weights: False
  expansion_method: expansion_layer
  expansion_ratio: 2
  decoding_strategy: prefix
  config:
    num_hidden_layers: 8
    hidden_size: 2048

token_decoding_loss:
  enable: true

block_decoding_loss:
  enable: false
  weight: 1.0

auto_encoding_loss:
  enable: false
  weight: 1.0

adaptive_lr_auto_encoder: null

freeze: false
learning_rate: 3.0e-4
adam_beta1: 0.9
adam_beta2: 0.95
weight_decay: 0.1
precision: bf16
num_train_steps: 286000
stop_steps: 286000
num_warmup_steps: 1500
save_steps: 5000
save_total_limit: null
logging_steps: 100
dataloader_num_workers: 8

deepspeed: ds_configs/default_linear_warmup.config

zero_shot_eval:
  enable: true
  tasks: lambada_openai,wikitext,hellaswag,arc_easy,sciq
  eval_steps: 10000
